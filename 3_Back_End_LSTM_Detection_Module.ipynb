{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ubuntu/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "#mnist = input_data.read_data_sets('MNIST_data', one_hot=True) # Removed\n",
    "import numpy as np # Included\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from numpy import genfromtxt\n",
    "benign_traffic = genfromtxt('/home/ubuntu/dataset/benign_traffic.csv', delimiter=',')\n",
    "gafgyt_combo = genfromtxt('/home/ubuntu/dataset/gafgyt_combo.csv', delimiter=',')\n",
    "gafgyt_junk = genfromtxt('/home/ubuntu/dataset/gafgyt_junk.csv', delimiter=',')\n",
    "gafgyt_scan = genfromtxt('/home/ubuntu/dataset/gafgyt_scan.csv', delimiter=',')\n",
    "gafgyt_tcp = genfromtxt('/home/ubuntu/dataset/gafgyt_tcp.csv', delimiter=',')\n",
    "gafgyt_udp = genfromtxt('/home/ubuntu/dataset/gafgyt_udp.csv', delimiter=',')\n",
    "mirai_ack = genfromtxt('/home/ubuntu/dataset/mirai_ack.csv', delimiter=',')\n",
    "mirai_scan = genfromtxt('/home/ubuntu/dataset/mirai_scan.csv', delimiter=',')\n",
    "mirai_syn = genfromtxt('/home/ubuntu/dataset/mirai_syn.csv', delimiter=',')\n",
    "mirai_udp = genfromtxt('/home/ubuntu/dataset/mirai_udp.csv', delimiter=',')\n",
    "mirai_udpplain = genfromtxt('/home/ubuntu/dataset/mirai_udpplain.csv', delimiter=',')\n",
    "\n",
    "# with open('/home/ubuntu/dataset/benign_traffic.csv', 'r') as csvfile:\n",
    "\n",
    "#     csvreader = csv.reader(csvfile)\n",
    "#     next(csvreader,None)\n",
    "#     output = open('/home/ubuntu/dataset/corrected_benign_traffic.csv', 'wb')\n",
    "#     writer = csv.writer(output)\n",
    "#     for row in csvreader:\n",
    "#         if row[1].isdigit()==True:\n",
    "#             print(row)\n",
    "#             #writer.writerow(row)\n",
    "#     csvfile.close()\n",
    "#     output.close()\n",
    "\n",
    "#benign_traffic = pd.read_csv(\"/home/ubuntu/dataset/benign_traffic.csv\")\n",
    "\n",
    "#benign_traffic.head()\n",
    "# fx_array=open('Parsed Data/complete_train_rand_xin.npy','rb')\n",
    "# x_in = np.load(fx_array)\n",
    "\n",
    "# fy_array=open('Parsed Data/complete_train_rand_yin.npy','rb')\n",
    "# y_in = np.load(fy_array)\n",
    "\n",
    "# fxt_array=open('Parsed Data/complete_test_rand_xin.npy','rb')\n",
    "# xt_in = np.load(fxt_array)\n",
    "\n",
    "# fyt_array=open('Parsed Data/complete_test_rand_yin.npy','rb')\n",
    "# yt_in = np.load(fyt_array)\n",
    "\n",
    "\n",
    "# print(\"Normal Sample: \\n\")\n",
    "# print(\" Method: 0\\n \\\n",
    "# Content_length:0 \\n \\\n",
    "# URL: http://localhost:8080/tienda1/miembros/editar.jsp \\n \\\n",
    "# Payload: direccion=Calle+Barrio+Cura-vigo+Vello%2C+134+10%3FD \\n\")\n",
    "\n",
    "# print(xt_in[84])\n",
    "# print(\"\\n\")\n",
    "# print(yt_in[84])\n",
    "# print(\"\\n\")\n",
    "\n",
    "# print(\"Abormal Sample:\")\n",
    "# print(\"\\n\")\n",
    "# print(\" Method: 0\\n \\\n",
    "# Content_length:0 \\n \\\n",
    "# URL: http://localhost:8080/tienda1/publico/caracteristicas.jsp \\n\\n \\\n",
    "# Payload: id=1sessionid%3D12312312%26+username%3D%253C%2573%2563%2572%2569%2570%2574%253E%2564%256F%2563%2575%256D%2565%256E%2574%252E%256C%256F%2563%2561%2574%2569%256F%256E%253D%2527%2568%2574%2574%2570%253A%252F%252F%2561%2574%2574%2561%2563%256B%2565%2572%2568%256F%2573%2574%252E%2565%2578%2561%256D%2570%256C%2565%252F%2563%2567%2569%252D%2562%2569%256E%252F%2563%256F%256F%256B%2569%2565%2573%2574%2565%2561%256C%252E%2563%2567%2569%253F%2527%252B%2564%256F%2563%2575%256D%2565%256E%2574%252E%2563%256F%256F%256B%2569%2565%253C%252F%2573+%2563%2572%2569%2570%2574%253E%3F \\n\\n \\\n",
    "# Payload Decoded: id 1sessionid 12312312 username script document location http attackerhost example cgi bin cookiesteal cgi document cookie s cript 0 0\")\n",
    "# print(\"\\n\")\n",
    "# print(xt_in[337])\n",
    "# print(\"\\n\")\n",
    "# print(yt_in[337])\n",
    "\n",
    "# test_xin_np = np.array([xt_in[84],xt_in[337]])\n",
    "# test_yin_np = np.array([yt_in[84],yt_in[337]])\n",
    "\n",
    "# print(\"Input Test Data:\")\n",
    "# print(type(xt_in))\n",
    "# print(len(xt_in))\n",
    "# print(type(xt_in[0]))\n",
    "# print(len(xt_in[0]))\n",
    "\n",
    "# print(\"Input Test Data:\")\n",
    "# print(type(test_xin_np))\n",
    "# print(len(test_xin_np))\n",
    "# print(type(test_xin_np[0]))\n",
    "# print(len(test_xin_np[0]))\n",
    "# print(test_xin_np[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(benign_traffic[0:10])\n",
    "numrows = len(benign_traffic)    # 3 rows in your example\n",
    "numcols = len(benign_traffic[0]) # 2 columns in your example\n",
    "print(\"Dimensions Rows: %s Columns: %s\" %(numrows,numcols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(benign_traffic[62660:62666])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_traffic = np.delete(benign_traffic, [0,49549, 62663,101764,277005,339160,437675,489826,526412], axis=0)\n",
    "gafgyt_combo = np.delete(gafgyt_combo, [0,59719,112732,165747,223900,285281,334812,401482,455766], axis=0)\n",
    "gafgyt_junk = np.delete(gafgyt_junk, [0,], axis=0)\n",
    "gafgyt_scan = np.delete(gafgyt_scan, [0,], axis=0)\n",
    "gafgyt_tcp = np.delete(gafgyt_tcp, [0,], axis=0)\n",
    "gafgyt_udp = np.delete(gafgyt_udp, [0,], axis=0)\n",
    "mirai_ack = np.delete(mirai_ack, [0,], axis=0)\n",
    "mirai_scan = np.delete(mirai_scan, [0,], axis=0)\n",
    "mirai_syn = np.delete(mirai_syn, [0,], axis=0)\n",
    "mirai_udp = np.delete(mirai_udp, [0,], axis=0)\n",
    "mirai_udpplain = np.delete(mirai_udpplain, [0,], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#benign_traffic[~np.isnan(benign_traffic).any(axis=1)]\n",
    "print(benign_traffic[0:10])\n",
    "numrows = len(benign_traffic)    # 3 rows in your example\n",
    "numcols = len(benign_traffic[0]) # 2 columns in your example\n",
    "print(\"Dimensions Rows: %s Columns: %s\" %(numrows,numcols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(benign_traffic[62660:62666])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hm_epochs = 10\n",
    "n_classes = 2\n",
    "batch_size = 128\n",
    "chunk_size = 1 # Changed\n",
    "n_chunks = 115 # Number of attributes in dataset\n",
    "rnn_size = 30\n",
    "\n",
    "x = tf.placeholder(\"float\", shape=[None, n_chunks, chunk_size])\n",
    "y = tf.placeholder(\"float\")\n",
    "x_t = tf.placeholder(\"float\", shape=[None, n_chunks, chunk_size])\n",
    "y_t = tf.placeholder(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# x_request = tf.reshape(x, [-1,28,1,1])\n",
    "# print (\"x_request=\")\n",
    "# print (x_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class DataSet(object):\n",
    "    def __init__(self, requests, labels):\n",
    "        assert requests.shape[0] == labels.shape[0], (\"requests.shape: %s labels.shape: %s\" % (requests.shape,\n",
    "                                                 labels.shape))\n",
    "        self._num_examples = requests.shape[0]\n",
    "        # Convert shape from [num examples, rows, columns, depth]\n",
    "        # to [num examples, rows*columns] (assuming depth == 1)\n",
    "        requests = requests.reshape(requests.shape[0],\n",
    "                              requests.shape[1])\n",
    "        self._requests = requests\n",
    "        self._labels = labels\n",
    "        self._epochs_completed = 0\n",
    "        self._index_in_epoch = 0\n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "        start = self._index_in_epoch\n",
    "        self._index_in_epoch += batch_size\n",
    "        if self._index_in_epoch > self._num_examples:\n",
    "            # Finished epoch\n",
    "            self._epochs_completed += 1\n",
    "            # Shuffle the data\n",
    "            #print(\"=== Current Perm ===\")\n",
    "            #print(\"=== index_in_epoch === : %s\" % self._index_in_epoch)\n",
    "            perm = np.arange(self._num_examples)\n",
    "            #print(perm)\n",
    "            np.random.shuffle(perm)\n",
    "            self._requests = self._requests[perm]\n",
    "            self._labels = self._labels[perm]\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size\n",
    "            assert batch_size <= self._num_examples\n",
    "        end = self._index_in_epoch\n",
    "        return self._requests[start:end], self._labels[start:end]\n",
    "\n",
    "current_data = DataSet(x_in, y_in)\n",
    "def recurrent_neural_network(x):\n",
    "    layer = {'weights':tf.Variable(tf.random_normal([rnn_size, n_classes])),\n",
    "             'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
    "    \n",
    "    x = tf.transpose(x, [1,0,2])\n",
    "    x = tf.reshape(x, [-1, chunk_size])\n",
    "    x = tf.split(0, n_chunks, x)\n",
    "    def make_cell():\n",
    "        \n",
    "        lstm_cell = rnn_cell.BasicLSTMCell(rnn_size)\n",
    "        return lstm_cell\n",
    "    \n",
    "    lstm_cell = tf.contrib.rnn.MultiRNNCell(\n",
    "    [make_cell for _ in range(2)], state_is_tuple=True)\n",
    "    \n",
    "    \n",
    "    #Helper\n",
    "    #Decoder\n",
    "    # +  = dynamic\n",
    "    output, states = rnn.rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    output = tf.matmul(output[-1],layer['weights']) + layer['biases']\n",
    "\n",
    "    return output\n",
    "\n",
    "def train_neural_network(x):\n",
    "    prediction = recurrent_neural_network(x)\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(prediction,y) )\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            for _ in range(int(current_data._num_examples/batch_size)):\n",
    "                epoch_x, epoch_y = current_data.next_batch(batch_size)\n",
    "                epoch_x = epoch_x.reshape((batch_size, n_chunks, chunk_size))\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                epoch_loss += c\n",
    "\n",
    "            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "\n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('Accuracy:',accuracy.eval({x:xt_in.reshape((-1, n_chunks, chunk_size)), y:yt_in}))\n",
    "        #print('Test Abnormal:',accuracy.eval({x_t:test_xin_np.reshape((-1, n_chunks, chunk_size)), y_t:test_yin_np}))\n",
    "\n",
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
