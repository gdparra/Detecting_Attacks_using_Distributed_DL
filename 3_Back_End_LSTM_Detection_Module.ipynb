{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "#mnist = input_data.read_data_sets('MNIST_data', one_hot=True) # Removed\n",
    "import numpy as np # Included\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction import DictVectorizer as DV\n",
    "\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "dir1=\"/home/ubuntu/dataset/\"\n",
    "#dir1=\"/Users/ejl334/Downloads/IoT Dataset by UC/Concatenated/Archive/\"\n",
    "\n",
    "from numpy import genfromtxt\n",
    "benign_traffic = genfromtxt(dir1+'benign_traffic.csv', delimiter=',')\n",
    "gafgyt_combo = genfromtxt(dir1+'gafgyt_combo.csv', delimiter=',')\n",
    "# gafgyt_junk = genfromtxt('/home/ubuntu/dataset/gafgyt_junk.csv', delimiter=',')\n",
    "# gafgyt_scan = genfromtxt('/home/ubuntu/dataset/gafgyt_scan.csv', delimiter=',')\n",
    "# gafgyt_tcp = genfromtxt('/home/ubuntu/dataset/gafgyt_tcp.csv', delimiter=',')\n",
    "# gafgyt_udp = genfromtxt('/home/ubuntu/dataset/gafgyt_udp.csv', delimiter=',')\n",
    "# mirai_ack = genfromtxt('/home/ubuntu/dataset/mirai_ack.csv', delimiter=',')\n",
    "# mirai_scan = genfromtxt('/home/ubuntu/dataset/mirai_scan.csv', delimiter=',')\n",
    "# mirai_syn = genfromtxt('/home/ubuntu/dataset/mirai_syn.csv', delimiter=',')\n",
    "# mirai_udp = genfromtxt('/home/ubuntu/dataset/mirai_udp.csv', delimiter=',')\n",
    "# mirai_udpplain = genfromtxt('/home/ubuntu/dataset/mirai_udpplain.csv', delimiter=',')\n",
    "\n",
    "# with open('/home/ubuntu/dataset/benign_traffic.csv', 'r') as csvfile:\n",
    "\n",
    "#     csvreader = csv.reader(csvfile)\n",
    "#     next(csvreader,None)\n",
    "#     output = open('/home/ubuntu/dataset/corrected_benign_traffic.csv', 'wb')\n",
    "#     writer = csv.writer(output)\n",
    "#     for row in csvreader:\n",
    "#         if row[1].isdigit()==True:\n",
    "#             print(row)\n",
    "#             #writer.writerow(row)\n",
    "#     csvfile.close()\n",
    "#     output.close()\n",
    "\n",
    "#benign_traffic = pd.read_csv(\"/home/ubuntu/dataset/benign_traffic.csv\")\n",
    "\n",
    "#benign_traffic.head()\n",
    "# fx_array=open('Parsed Data/complete_train_rand_xin.npy','rb')\n",
    "# x_in = np.load(fx_array)\n",
    "\n",
    "# fy_array=open('Parsed Data/complete_train_rand_yin.npy','rb')\n",
    "# y_in = np.load(fy_array)\n",
    "\n",
    "# fxt_array=open('Parsed Data/complete_test_rand_xin.npy','rb')\n",
    "# xt_in = np.load(fxt_array)\n",
    "\n",
    "# fyt_array=open('Parsed Data/complete_test_rand_yin.npy','rb')\n",
    "# yt_in = np.load(fyt_array)\n",
    "\n",
    "\n",
    "# print(\"Normal Sample: \\n\")\n",
    "# print(\" Method: 0\\n \\\n",
    "# Content_length:0 \\n \\\n",
    "# URL: http://localhost:8080/tienda1/miembros/editar.jsp \\n \\\n",
    "# Payload: direccion=Calle+Barrio+Cura-vigo+Vello%2C+134+10%3FD \\n\")\n",
    "\n",
    "# print(xt_in[84])\n",
    "# print(\"\\n\")\n",
    "# print(yt_in[84])\n",
    "# print(\"\\n\")\n",
    "\n",
    "# print(\"Abormal Sample:\")\n",
    "# print(\"\\n\")\n",
    "# print(\" Method: 0\\n \\\n",
    "# Content_length:0 \\n \\\n",
    "# URL: http://localhost:8080/tienda1/publico/caracteristicas.jsp \\n\\n \\\n",
    "# Payload: id=1sessionid%3D12312312%26+username%3D%253C%2573%2563%2572%2569%2570%2574%253E%2564%256F%2563%2575%256D%2565%256E%2574%252E%256C%256F%2563%2561%2574%2569%256F%256E%253D%2527%2568%2574%2574%2570%253A%252F%252F%2561%2574%2574%2561%2563%256B%2565%2572%2568%256F%2573%2574%252E%2565%2578%2561%256D%2570%256C%2565%252F%2563%2567%2569%252D%2562%2569%256E%252F%2563%256F%256F%256B%2569%2565%2573%2574%2565%2561%256C%252E%2563%2567%2569%253F%2527%252B%2564%256F%2563%2575%256D%2565%256E%2574%252E%2563%256F%256F%256B%2569%2565%253C%252F%2573+%2563%2572%2569%2570%2574%253E%3F \\n\\n \\\n",
    "# Payload Decoded: id 1sessionid 12312312 username script document location http attackerhost example cgi bin cookiesteal cgi document cookie s cript 0 0\")\n",
    "# print(\"\\n\")\n",
    "# print(xt_in[337])\n",
    "# print(\"\\n\")\n",
    "# print(yt_in[337])\n",
    "\n",
    "# test_xin_np = np.array([xt_in[84],xt_in[337]])\n",
    "# test_yin_np = np.array([yt_in[84],yt_in[337]])\n",
    "\n",
    "# print(\"Input Test Data:\")\n",
    "# print(type(xt_in))\n",
    "# print(len(xt_in))\n",
    "# print(type(xt_in[0]))\n",
    "# print(len(xt_in[0]))\n",
    "\n",
    "# print(\"Input Test Data:\")\n",
    "# print(type(test_xin_np))\n",
    "# print(len(test_xin_np))\n",
    "# print(type(test_xin_np[0]))\n",
    "# print(len(test_xin_np[0]))\n",
    "# print(test_xin_np[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================\n",
      "***BENIGN TRAFFIC***\n",
      "========================================================================\n",
      "***DIMENSIONS***\n",
      "Dimensions Rows: 555941 Columns: 115\n",
      "***NAN ROWS***\n",
      "[[     0]\n",
      " [ 59719]\n",
      " [112732]\n",
      " [165747]\n",
      " [223900]\n",
      " [285281]\n",
      " [342812]\n",
      " [401482]\n",
      " [455766]]\n",
      "***** NAN Removal *****\n",
      "***DIMENSIONS***\n",
      "Dimensions Rows: 555932 Columns: 115\n",
      "***NAN ROWS***\n",
      "[]\n",
      "========================================================================\n",
      "***MALICIOUS TRAFFIC***\n",
      "========================================================================\n",
      "***DIMENSIONS***\n",
      "Dimensions Rows: 515165 Columns: 115\n",
      "***NAN ROWS***\n",
      "[[     0]\n",
      " [ 59719]\n",
      " [112732]\n",
      " [165747]\n",
      " [223900]\n",
      " [285281]\n",
      " [342812]\n",
      " [401482]\n",
      " [455766]]\n",
      "***** NAN Removal *****\n",
      "***DIMENSIONS***\n",
      "Dimensions Rows: 515156 Columns: 115\n",
      "***NAN ROWS***\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# DELETE COLUMNS WITH NAN\n",
    "print(\"========================================================================\")\n",
    "print(\"***BENIGN TRAFFIC***\")\n",
    "print(\"========================================================================\")\n",
    "print(\"***DIMENSIONS***\")\n",
    "numrows = len(benign_traffic)\n",
    "numcols = len(benign_traffic[0])\n",
    "print(\"Dimensions Rows: %s Columns: %s\" %(numrows,numcols))\n",
    "\n",
    "print(\"***NAN ROWS***\")\n",
    "nan_rows=np.argwhere(np.isnan(gafgyt_combo).any(axis=1))\n",
    "print(nan_rows)\n",
    "\n",
    "print(\"***** NAN Removal *****\")\n",
    "benign_traffic = benign_traffic[~np.isnan(benign_traffic).any(axis=1)]\n",
    "\n",
    "print(\"***DIMENSIONS***\")\n",
    "numrows_good = len(benign_traffic)\n",
    "numcols = len(benign_traffic[0])\n",
    "print(\"Dimensions Rows: %s Columns: %s\" %(numrows_good,numcols))\n",
    "\n",
    "print(\"***NAN ROWS***\")\n",
    "nan_rows=np.argwhere(np.isnan(benign_traffic).any(axis=1))\n",
    "print(nan_rows)\n",
    "\n",
    "print(\"========================================================================\")\n",
    "print(\"***MALICIOUS TRAFFIC***\")\n",
    "print(\"========================================================================\")\n",
    "print(\"***DIMENSIONS***\")\n",
    "malicious_dataset = gafgyt_combo\n",
    "\n",
    "numrows = len(malicious_dataset)\n",
    "numcols = len(malicious_dataset[0])\n",
    "print(\"Dimensions Rows: %s Columns: %s\" %(numrows,numcols))\n",
    "\n",
    "print(\"***NAN ROWS***\")\n",
    "nan_rows=np.argwhere(np.isnan(malicious_dataset).any(axis=1))\n",
    "print(nan_rows)\n",
    "\n",
    "print(\"***** NAN Removal *****\")\n",
    "malicious_dataset = malicious_dataset[~np.isnan(malicious_dataset).any(axis=1)]\n",
    "\n",
    "print(\"***DIMENSIONS***\")\n",
    "numrows_bad = len(malicious_dataset)\n",
    "numcols = len(malicious_dataset[0])\n",
    "print(\"Dimensions Rows: %s Columns: %s\" %(numrows_bad,numcols))\n",
    "print(\"***NAN ROWS***\")\n",
    "nan_rows=np.argwhere(np.isnan(malicious_dataset).any(axis=1))\n",
    "print(nan_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 6.00000000e+01, 0.00000000e+00, 1.00000000e+00,\n",
       "        6.00000000e+01, 0.00000000e+00, 1.00000000e+00, 6.00000000e+01,\n",
       "        0.00000000e+00, 1.00000000e+00, 6.00000000e+01, 0.00000000e+00,\n",
       "        1.00000000e+00, 6.00000000e+01, 0.00000000e+00, 1.00000000e+00,\n",
       "        6.00000000e+01, 0.00000000e+00, 1.00000000e+00, 6.00000000e+01,\n",
       "        0.00000000e+00, 1.00000000e+00, 6.00000000e+01, 0.00000000e+00,\n",
       "        1.00000000e+00, 6.00000000e+01, 0.00000000e+00, 1.00000000e+00,\n",
       "        6.00000000e+01, 0.00000000e+00, 1.00000000e+00, 6.00000000e+01,\n",
       "        0.00000000e+00, 6.00000000e+01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.00000000e+00, 6.00000000e+01, 0.00000000e+00,\n",
       "        6.00000000e+01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.00000000e+00, 6.00000000e+01, 0.00000000e+00, 6.00000000e+01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "        6.00000000e+01, 0.00000000e+00, 6.00000000e+01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 6.00000000e+01,\n",
       "        0.00000000e+00, 6.00000000e+01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.00000000e+00, 1.50566169e+09, 0.00000000e+00,\n",
       "        1.00000000e+00, 1.50566169e+09, 0.00000000e+00, 1.00000000e+00,\n",
       "        1.50566169e+09, 0.00000000e+00, 1.00000000e+00, 1.50566169e+09,\n",
       "        0.00000000e+00, 1.00000000e+00, 1.50566169e+09, 0.00000000e+00,\n",
       "        1.00000000e+00, 6.00000000e+01, 0.00000000e+00, 6.00000000e+01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "        6.00000000e+01, 0.00000000e+00, 6.00000000e+01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 6.00000000e+01,\n",
       "        0.00000000e+00, 6.00000000e+01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.00000000e+00, 6.00000000e+01, 0.00000000e+00,\n",
       "        6.00000000e+01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.00000000e+00, 6.00000000e+01, 0.00000000e+00, 6.00000000e+01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.00000000e+00, 3.54000000e+02, 0.00000000e+00, 1.00000000e+00,\n",
       "        3.54000000e+02, 0.00000000e+00, 1.00000000e+00, 3.54000000e+02,\n",
       "        0.00000000e+00, 1.00000000e+00, 3.54000000e+02, 0.00000000e+00,\n",
       "        1.00000000e+00, 3.54000000e+02, 0.00000000e+00, 1.00000003e+00,\n",
       "        3.54000000e+02, 4.59000000e-06, 1.00003178e+00, 3.53999619e+02,\n",
       "        4.57538300e-03, 1.03175707e+00, 3.53630645e+02, 4.29583941e+00,\n",
       "        2.59751522e+00, 3.46619800e+02, 3.40950471e+01, 5.31989524e+00,\n",
       "        3.44262695e+02, 2.21882986e+01, 1.00000003e+00, 3.54000000e+02,\n",
       "        2.14251900e-03, 3.54000000e+02, 4.59000000e-06, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.00003178e+00, 3.53999619e+02, 6.76415740e-02,\n",
       "        3.53999619e+02, 4.57538300e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.03175707e+00, 3.53630645e+02, 2.07264069e+00, 3.53630645e+02,\n",
       "        4.29583941e+00, 0.00000000e+00, 0.00000000e+00, 2.59751522e+00,\n",
       "        3.46619800e+02, 5.83909643e+00, 3.46619800e+02, 3.40950471e+01,\n",
       "        0.00000000e+00, 0.00000000e+00, 5.31989524e+00, 3.44262695e+02,\n",
       "        4.71044569e+00, 3.44262695e+02, 2.21882986e+01, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.00000003e+00, 4.98057520e+00, 4.23000000e-07,\n",
       "        1.00003178e+00, 4.98069089e+00, 4.22029000e-04, 1.03175707e+00,\n",
       "        5.09251142e+00, 4.18370441e-01, 2.59751522e+00, 3.12448488e+01,\n",
       "        6.97610498e+03, 5.31989524e+00, 5.80165110e+01, 1.27406361e+04,\n",
       "        1.00000003e+00, 3.54000000e+02, 2.14251900e-03, 3.54000000e+02,\n",
       "        4.59000000e-06, 0.00000000e+00, 0.00000000e+00, 1.00003178e+00,\n",
       "        3.53999619e+02, 6.76415740e-02, 3.53999619e+02, 4.57538300e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.03175707e+00, 3.53630645e+02,\n",
       "        2.07264069e+00, 3.53630645e+02, 4.29583941e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.59751522e+00, 3.46619800e+02, 5.83909643e+00,\n",
       "        3.46619800e+02, 3.40950471e+01, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.31989524e+00, 3.44262695e+02, 4.71044569e+00, 3.44262695e+02,\n",
       "        2.21882986e+01, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benign_traffic[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE UNWANTED COLUMNS\n",
    "benign_traffic = np.delete(benign_traffic, [2,5,8,11,14,17,20,23,26,29,\\\n",
    "                                            34,35,41,42,48,49,55,56,62,63,\\\n",
    "                                            66,67,69,70,72,73,75,76,78,79,\\\n",
    "                                            84,85,91,92,98,99,105,106,112,113,], axis=1)\n",
    "malicious_dataset = np.delete(malicious_dataset, [2,5,8,11,14,17,20,23,26,29,\\\n",
    "                                            34,35,41,42,48,49,55,56,62,63,\\\n",
    "                                            66,67,69,70,72,73,75,76,78,79,\\\n",
    "                                            84,85,91,92,98,99,105,106,112,113,], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***DIMENSIONS BENIGN***\n",
      "Dimensions Rows: 555932 Columns: 75\n",
      "***DIMENSIONS MALICIOUS***\n",
      "Dimensions Rows: 515156 Columns: 75\n"
     ]
    }
   ],
   "source": [
    "print(\"***DIMENSIONS BENIGN***\")\n",
    "numrows_good = len(benign_traffic)\n",
    "numcols_good = len(benign_traffic[0])\n",
    "print(\"Dimensions Rows: %s Columns: %s\" %(numrows_good,numcols_good))\n",
    "print(\"***DIMENSIONS MALICIOUS***\")\n",
    "numrows_bad = len(malicious_dataset)\n",
    "numcols_bad = len(malicious_dataset[0])\n",
    "print(\"Dimensions Rows: %s Columns: %s\" %(numrows_bad,numcols_bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** COMPLETE DATASET ***\n",
      "Dimensions Rows: 1071088 Columns: 75\n"
     ]
    }
   ],
   "source": [
    "complete_dataset = np.concatenate((benign_traffic, malicious_dataset), 0)\n",
    "print(\"*** COMPLETE DATASET ***\")\n",
    "numrows = len(complete_dataset)\n",
    "numcols = len(complete_dataset[0])\n",
    "print(\"Dimensions Rows: %s Columns: %s\" %(numrows,numcols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZE DATASET\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataX_ = scaler.fit_transform(complete_dataset)\n",
    "#dataX_ = scaler.fit_transform(np.reshape(complete_dataset, (numrows, numcols)))\n",
    "#                                                        (5000,100*75)\n",
    "X_ = dataX_\n",
    "#X_ = np.reshape(dataX_, (numrows, numcols))\n",
    "#X_ = np.reshape(dataX_, (5000, 100, 75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** COMPLETE DATASET ***\n",
      "Dimensions Rows: 555932 Columns: 75\n",
      "*** COMPLETE DATASET ***\n",
      "Dimensions Rows: 515156 Columns: 75\n"
     ]
    }
   ],
   "source": [
    "benign_traffic = X_[:numrows_good, :]\n",
    "malicious_dataset = X_[numrows_good:, :]\n",
    "\n",
    "print(\"*** COMPLETE DATASET ***\")\n",
    "numrows = len(benign_traffic)\n",
    "numcols = len(benign_traffic[0])\n",
    "print(\"Dimensions Rows: %s Columns: %s\" %(numrows,numcols))\n",
    "\n",
    "print(\"*** COMPLETE DATASET ***\")\n",
    "numrows = len(malicious_dataset)\n",
    "numcols = len(malicious_dataset[0])\n",
    "print(\"Dimensions Rows: %s Columns: %s\" %(numrows,numcols))\n",
    "\n",
    "# Clear Variables\n",
    "del X_\n",
    "del dataX_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444746\n",
      "412125\n"
     ]
    }
   ],
   "source": [
    "def rounder(x):\n",
    "    if (x-int(x) >= 0.5):\n",
    "        return np.ceil(x)\n",
    "    else:\n",
    "        return np.floor(x)\n",
    "\n",
    "split1 = int(rounder(len(benign_traffic)*0.8))\n",
    "split2 = int(rounder(len(malicious_dataset)*0.8))\n",
    "current_bot_data = gafgyt_combo\n",
    "\n",
    "print(split1)\n",
    "print(split2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** DIMENSIONS benign_pre_train_x***\n",
      "Dimensions Rows: 444746 Columns: 75\n",
      "*** DIMENSIONS benign_pre_test_x***\n",
      "Dimensions Rows: 111186 Columns: 75\n",
      "*** DIMENSIONS benign_pre_train_y***\n",
      "Dimensions Rows: 444746 Columns: 75\n",
      "*** DIMENSIONS benign_pre_test_y***\n",
      "Dimensions Rows: 111186 Columns: 75\n",
      "*** DIMENSIONS malicious_pre_train_x***\n",
      "Dimensions Rows: 412125 Columns: 75\n",
      "*** DIMENSIONS malicious_pre_test_x***\n",
      "Dimensions Rows: 103031 Columns: 75\n",
      "*** DIMENSIONS malicious_pre_train_y***\n",
      "Dimensions Rows: 412125 Columns: 75\n",
      "*** DIMENSIONS malicious_pre_test_y***\n",
      "Dimensions Rows: 103031 Columns: 75\n",
      "*** DIMENSIONS - TRAIN_X ***\n",
      "Dimensions Rows: 856871 Columns: 75\n",
      "*** DIMENSIONS TEXT_X ***\n",
      "Dimensions Rows: 214217 Columns: 75\n",
      "*** DIMENSIONS TRAIN_Y ***\n",
      "Dimensions Rows: 856871 Columns: 75\n",
      "*** DIMENSIONS TEST_Y***\n",
      "Dimensions Rows: 214217 Columns: 75\n"
     ]
    }
   ],
   "source": [
    "# benign_traffic = np.delete(benign_traffic, [0,49549,62663,101764,277005,339160,437675,489826,526412], axis=0)\n",
    "# gafgyt_combo = np.delete(gafgyt_combo, [0,59719,112732,165747,223900,285281,334812,401482,455766], axis=0)\n",
    "# gafgyt_junk = np.delete(gafgyt_junk, [0,29069,59382,89180,117530,148429,177498,205804,234384], axis=0)\n",
    "# gafgyt_scan = np.delete(gafgyt_scan, [0,29850,57345,85466,113326,142624,171022,198721,226547], axis=0)\n",
    "# gafgyt_tcp = np.delete(gafgyt_tcp, [0,92142,187164,288701,381283,485794,575182,672966,761783], axis=0)\n",
    "# gafgyt_udp = np.delete(gafgyt_udp, [0,105875,210667,314601,420384,524396,629055,739673,843394], axis=0)\n",
    "# mirai_ack = np.delete(mirai_ack, [0,102196,215482,306606,367161,425159,536640], axis=0)\n",
    "# mirai_scan = np.delete(mirai_scan, [0,107686,150879,254501,351283,448380,494311], axis=0)\n",
    "# mirai_syn = np.delete(mirai_syn, [0,122574,239382,357511,423258,485110,610826], axis=0)\n",
    "# mirai_udp = np.delete(mirai_udp, [0,], axis=0)\n",
    "# mirai_udpplain = np.delete(mirai_udpplain, [0,], axis=0)\n",
    "\n",
    "def dimensions_data(x):\n",
    "    numrows = len(x)\n",
    "    numcols = len(x[0])\n",
    "    print(\"Dimensions Rows: %s Columns: %s\" %(numrows,numcols))\n",
    "\n",
    "y_benings = np.zeros(len(benign_traffic),dtype=int)\n",
    "y_malicious = np.ones(len(malicious_dataset),dtype=int)\n",
    "\n",
    "benign_pre_train_x = benign_traffic[:split1, :]\n",
    "print(\"*** DIMENSIONS benign_pre_train_x***\")\n",
    "dimensions_data(benign_pre_train_x)\n",
    "\n",
    "benign_pre_test_x = benign_traffic[split1:, :]\n",
    "print(\"*** DIMENSIONS benign_pre_test_x***\")\n",
    "dimensions_data(benign_pre_test_x)\n",
    "\n",
    "benign_pre_train_y = benign_traffic[:split1, :]\n",
    "print(\"*** DIMENSIONS benign_pre_train_y***\")\n",
    "dimensions_data(benign_pre_train_y)\n",
    "\n",
    "benign_pre_test_y = benign_traffic[split1:, :]\n",
    "print(\"*** DIMENSIONS benign_pre_test_y***\")\n",
    "dimensions_data(benign_pre_test_y)\n",
    "\n",
    "\n",
    "\n",
    "malicious_pre_train_x = malicious_dataset[:split2, :]\n",
    "print(\"*** DIMENSIONS malicious_pre_train_x***\")\n",
    "dimensions_data(malicious_pre_train_x\n",
    "               )\n",
    "malicious_pre_test_x = malicious_dataset[split2:, :]\n",
    "print(\"*** DIMENSIONS malicious_pre_test_x***\")\n",
    "dimensions_data(malicious_pre_test_x)\n",
    "\n",
    "malicious_pre_train_y = malicious_dataset[:split2, :]\n",
    "print(\"*** DIMENSIONS malicious_pre_train_y***\")\n",
    "dimensions_data(malicious_pre_train_y)\n",
    "\n",
    "malicious_pre_test_y = malicious_dataset[split2:, :]\n",
    "print(\"*** DIMENSIONS malicious_pre_test_y***\")\n",
    "dimensions_data(malicious_pre_test_y)\n",
    "\n",
    "train_x = np.concatenate((benign_pre_train_x, malicious_pre_train_x), 0)\n",
    "print(\"*** DIMENSIONS - TRAIN_X ***\")\n",
    "dimensions_data(train_x)\n",
    "    \n",
    "test_x = np.concatenate((benign_pre_test_x, malicious_pre_test_x), 0)\n",
    "print(\"*** DIMENSIONS TEXT_X ***\")\n",
    "dimensions_data(test_x)\n",
    "\n",
    "train_y = np.concatenate((benign_pre_train_y,malicious_pre_train_y),0)\n",
    "print(\"*** DIMENSIONS TRAIN_Y ***\")\n",
    "dimensions_data(train_y)\n",
    "\n",
    "test_y = np.concatenate((benign_pre_test_y,malicious_pre_test_y),0)\n",
    "print(\"*** DIMENSIONS TEST_Y***\")\n",
    "dimensions_data(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  4.27885519e-01 0.00000000e+00 6.93889390e-18 0.00000000e+00\n",
      "  0.00000000e+00 4.23431207e-01 0.00000000e+00 6.93889390e-18\n",
      "  0.00000000e+00 0.00000000e+00 4.05812719e-01 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.73886056e-01\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  3.84832906e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 6.93889390e-18\n",
      "  0.00000000e+00 0.00000000e+00 3.70234072e-01 0.00000000e+00\n",
      "  6.93889390e-18 0.00000000e+00 0.00000000e+00 3.70841959e-01\n",
      "  0.00000000e+00 6.93889390e-18 0.00000000e+00 0.00000000e+00\n",
      "  3.66963499e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 3.64942629e-01 0.00000000e+00 2.08166817e-17\n",
      "  0.00000000e+00 0.00000000e+00 4.44787454e-01]\n",
      " [0.00000000e+00 2.11101180e-01 0.00000000e+00 2.11399766e-01\n",
      "  0.00000000e+00 2.13965140e-01 0.00000000e+00 2.19077035e-01\n",
      "  0.00000000e+00 2.22081959e-01 1.05477549e-10 2.11101180e-01\n",
      "  7.80916051e-08 2.11399492e-01 3.41625371e-05 2.13696334e-01\n",
      "  2.00612924e-04 2.13577605e-01 1.00999460e-04 2.14726585e-01\n",
      "  1.05508723e-10 2.08510638e-01 3.11185139e-06 1.62496011e-01\n",
      "  4.27885519e-01 7.81533785e-08 2.08510368e-01 9.77359398e-05\n",
      "  1.69051279e-01 4.23431207e-01 3.42147651e-05 2.08248684e-01\n",
      "  2.99340590e-03 2.08248684e-01 4.05812719e-01 2.01097415e-04\n",
      "  2.03276454e-01 8.60642332e-03 2.03276454e-01 3.73886056e-01\n",
      "  1.01307180e-04 2.01604748e-01 6.94309159e-03 2.01604748e-01\n",
      "  3.84832906e-01 1.05508723e-10 7.81533785e-08 3.42147651e-05\n",
      "  2.01097415e-04 1.01307180e-04 1.42742006e-10 2.08510638e-01\n",
      "  3.11185139e-06 1.62496011e-01 3.70234072e-01 1.16066219e-07\n",
      "  2.08510368e-01 9.77359398e-05 1.69051279e-01 3.70841959e-01\n",
      "  7.80914788e-05 2.08248684e-01 2.99340590e-03 2.08248684e-01\n",
      "  3.66963499e-01 2.51658664e-03 2.03276454e-01 8.50720814e-03\n",
      "  2.03276454e-01 3.64942629e-01 5.78107267e-03 2.01604748e-01\n",
      "  6.91694160e-03 2.01604748e-01 4.44787454e-01]]\n",
      "Dimensions Rows: 555932 Columns: 75\n"
     ]
    }
   ],
   "source": [
    "#benign_traffic[~np.isnan(benign_traffic).any(axis=1)]\n",
    "print(benign_traffic[0:2])\n",
    "numrows = len(benign_traffic)    # 3 rows in your example\n",
    "numcols = len(benign_traffic[0]) # 2 columns in your example\n",
    "print(\"Dimensions Rows: %s Columns: %s\" %(numrows,numcols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(856871, 75) (214217, 75)\n",
      "856871 214217\n",
      "57138008 14211342\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_x), np.shape(test_x))\n",
    "print(len(train_y), len(test_y))\n",
    "print(np.count_nonzero(train_y), np.count_nonzero(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(856871, 1, 75)\n",
      "(214217, 1, 75)\n",
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  4.27885519e-01 0.00000000e+00 6.93889390e-18 0.00000000e+00\n",
      "  0.00000000e+00 4.23431207e-01 0.00000000e+00 6.93889390e-18\n",
      "  0.00000000e+00 0.00000000e+00 4.05812719e-01 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.73886056e-01\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  3.84832906e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 6.93889390e-18\n",
      "  0.00000000e+00 0.00000000e+00 3.70234072e-01 0.00000000e+00\n",
      "  6.93889390e-18 0.00000000e+00 0.00000000e+00 3.70841959e-01\n",
      "  0.00000000e+00 6.93889390e-18 0.00000000e+00 0.00000000e+00\n",
      "  3.66963499e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 3.64942629e-01 0.00000000e+00 2.08166817e-17\n",
      "  0.00000000e+00 0.00000000e+00 4.44787454e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Vectorize data\n",
    "\n",
    "#vector_train_x = train_x.reshape((4061, 211, 75))\n",
    "vector_train_x = train_x.reshape((856871, 1, 75))\n",
    "print(vector_train_x.shape)\n",
    "#vector_test_x = test_x.reshape((12601, 17, 75))\n",
    "vector_test_x = test_x.reshape((214217, 1, 75))\n",
    "print(vector_test_x.shape)\n",
    "\n",
    "#vector_train_x = train_x.reshape(856871,75,np.newaxis)\n",
    "#vector_test_x = test_x.reshape(214217,75,np.newaxis)\n",
    "print(vector_train_x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch 1/10\n",
      " - 171s - loss: 2.6463e-04 - acc: 0.5736\n",
      "Epoch 2/10\n",
      " - 170s - loss: 3.2337e-05 - acc: 0.6033\n",
      "Epoch 3/10\n",
      " - 168s - loss: 2.5364e-05 - acc: 0.6205\n",
      "Epoch 4/10\n",
      " - 169s - loss: 2.2528e-05 - acc: 0.6419\n",
      "Epoch 5/10\n",
      " - 169s - loss: 1.9008e-05 - acc: 0.6537\n",
      "Epoch 6/10\n",
      " - 169s - loss: 1.6292e-05 - acc: 0.6471\n",
      "Epoch 7/10\n",
      " - 168s - loss: 1.5011e-05 - acc: 0.6526\n",
      "Epoch 8/10\n",
      " - 167s - loss: 1.4688e-05 - acc: 0.6560\n",
      "Epoch 9/10\n",
      " - 154s - loss: 1.4579e-05 - acc: 0.6581\n",
      "Epoch 10/10\n",
      " - 149s - loss: 1.4365e-05 - acc: 0.6624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f295d61fe80>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "batch_size = 28\n",
    "model = Sequential()\n",
    "model.add(LSTM(28, input_shape=(1,75), return_sequences=True))\n",
    "model.add(LSTM(28, input_shape=(1,75)))\n",
    "model.add(Dense(28))\n",
    "model.add(Dense(75))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])\n",
    "model.fit(vector_train_x, train_y, epochs=10, batch_size=batch_size, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hm_epochs = 30\n",
    "n_classes = 2\n",
    "batch_size = 10\n",
    "chunk_size = 1 # Changed\n",
    "n_chunks = 75 # Number of attributes in dataset\n",
    "rnn_size = 1\n",
    "\n",
    "x = tf.placeholder(\"float\", shape=[None, n_chunks, chunk_size])\n",
    "y = tf.placeholder(\"float\")\n",
    "x_t = tf.placeholder(\"float\", shape=[None, n_chunks, chunk_size])\n",
    "y_t = tf.placeholder(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# x_request = tf.reshape(x, [-1,28,1,1])\n",
    "# print (\"x_request=\")\n",
    "# print (x_request)\n",
    "#tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "static_rnn() got an unexpected keyword argument 'reuse'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-d10c74b54970>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m#print('Test Abnormal:',accuracy.eval({x_t:test_xin_np.reshape((-1, n_chunks, chunk_size)), y_t:test_yin_np}))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0mtrain_neural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-d10c74b54970>\u001b[0m in \u001b[0;36mtrain_neural_network\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_neural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecurrent_neural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-d10c74b54970>\u001b[0m in \u001b[0;36mrecurrent_neural_network\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m#     print(lstm_cell)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mlstm_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasicLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m#Helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: static_rnn() got an unexpected keyword argument 'reuse'"
     ]
    }
   ],
   "source": [
    "class DataSet(object):\n",
    "    def __init__(self, requests, labels):\n",
    "        assert requests.shape[0] == labels.shape[0], (\"requests.shape: %s labels.shape: %s\" % (requests.shape,\n",
    "                                                 labels.shape))\n",
    "        self._num_examples = requests.shape[0]\n",
    "        # Convert shape from [num examples, rows, columns, depth]\n",
    "        # to [num examples, rows*columns] (assuming depth == 1)\n",
    "        requests = requests.reshape(requests.shape[0],\n",
    "                              requests.shape[1])\n",
    "        self._requests = requests\n",
    "        self._labels = labels\n",
    "        self._epochs_completed = 0\n",
    "        self._index_in_epoch = 0\n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "        start = self._index_in_epoch\n",
    "        self._index_in_epoch += batch_size\n",
    "        if self._index_in_epoch > self._num_examples:\n",
    "            # Finished epoch\n",
    "            self._epochs_completed += 1\n",
    "            # Shuffle the data\n",
    "            #print(\"=== Current Perm ===\")\n",
    "            #print(\"=== index_in_epoch === : %s\" % self._index_in_epoch)\n",
    "            perm = np.arange(self._num_examples)\n",
    "            #print(perm)\n",
    "            np.random.shuffle(perm)\n",
    "            self._requests = self._requests[perm]\n",
    "            self._labels = self._labels[perm]\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size\n",
    "            assert batch_size <= self._num_examples\n",
    "        end = self._index_in_epoch\n",
    "        return self._requests[start:end], self._labels[start:end]\n",
    "\n",
    "current_data = DataSet(train_x, train_y)\n",
    "def recurrent_neural_network(x):\n",
    "    layer = {'weights':tf.Variable(tf.random_normal([rnn_size, n_classes])),\n",
    "             'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
    "    \n",
    "    x = tf.transpose(x, [1,0,2])\n",
    "    x = tf.reshape(x, [-1, chunk_size])\n",
    "    x = tf.split(x, n_chunks, 0)\n",
    "#     def make_cell():\n",
    "        \n",
    "#         lstm_cell = rnn_cell.BasicLSTMCell(rnn_size)\n",
    "#         return lstm_cell\n",
    "    \n",
    "#     lstm_cell = tf.contrib.rnn.MultiRNNCell(\n",
    "#     [make_cell for _ in range(2)], state_is_tuple=True)\n",
    "#     print(lstm_cell)\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(rnn_size) \n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "    \n",
    "    #Helper\n",
    "    #Decoder\n",
    "    # +  = dynamic\n",
    "    output, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    output = tf.matmul(output[-1],layer['weights']) + layer['biases']\n",
    "\n",
    "    return output\n",
    "\n",
    "def train_neural_network(x):\n",
    "    prediction = recurrent_neural_network(x)\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(plogits=prediction, labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            for _ in range(int(current_data._num_examples/batch_size)):\n",
    "                epoch_x, epoch_y = current_data.next_batch(batch_size)\n",
    "                epoch_x = epoch_x.reshape((batch_size, n_chunks, chunk_size))\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                epoch_loss += c\n",
    "\n",
    "            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "\n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('Accuracy:',accuracy.eval({x:test_x.reshape((-1, n_chunks, chunk_size)), y:test_y}))\n",
    "        #print('Test Abnormal:',accuracy.eval({x_t:test_xin_np.reshape((-1, n_chunks, chunk_size)), y_t:test_yin_np}))\n",
    "\n",
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
